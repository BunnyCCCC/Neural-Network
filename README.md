# Neural-Network
Mini projects about application of neural networks
------------------------------------------------------------------------------------------------------------




**Function Approximation**\
This project focused on experimenting function approximation using neural network regression models.
All of the different models used non-linear tanh activation function with different neuron sizes(2 to 50),
and different optimizers.
Compared Mean Squared Errors for three optimization methods, and CPU time with early stoppings
Finally concluded that 8 neurons in the hidden layer is the optimum size for this function approximation.
The optimizer of RMSprop with early stopping, batch size of 10 and trained for 1000 epoches achieved optimum 
result for this function approximation experiment.

**Multilayer Feedforward Network for Pattern Recognition**\
to do...

